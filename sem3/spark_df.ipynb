{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame API\n",
    "DataFrame is another Spark API which is very convinient for structured data.\n",
    "\n",
    "To use it, we need to instantiate a SparkSession, which is essentialy just enhaced SparkContext.\n",
    "It is created in similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2.1\"\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.10.4-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkConf = pyspark.SparkConf() \\\n",
    "    .set(\"spark.executor.memory\", \"2560m\")\\\n",
    "    .set(\"spark.driver.memory\", \"2560m\")\\\n",
    "    .set(\"spark.yarn.executor.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.yarn.driver.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.python.worker.memory\", \"1536m\")\\\n",
    "    .set(\"spark.executor.instances\", 11)\\\n",
    "    .set(\"spark.default.parallelism\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss = pyspark.sql.SparkSession.builder.config(conf=sparkConf).appName('seminar3-df').master('yarn').getOrCreate()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Web UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "port = ss.sparkContext.uiWebUrl.split(':')[-1]\n",
    "print 'http://cluster1:{}'.format(port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting the Data Files\n",
    "\n",
    "Download files if you did not do it in previous seminar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The KDD Cup 1999 competition dataset is described in detail \n",
    "[here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ! wget \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\" -O \"/data/kddcup.data_10_percent.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Put data into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ! hdfs dfs -put /data/kddcup.data_10_percent.gz ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A DataFrame is a Dataset organized into named columns. \n",
    "It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: csv, structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "To create one we utilize a DataFrameReader avaliable in SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = 'kddcup.data_10_percent.gz'\n",
    "data = ss.read.csv(data_path)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sometimes it's more convinient to use pandas dataframe representation in notebooks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We don't have column names in our data, but they are avaliable seperatley. Let's rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "header = requests.get('http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names').text.split('\\n')[1:-1]\n",
    "\n",
    "types = [h.split(':')[1].strip(' .') for h in header]\n",
    "header = [h.split(':')[0] for h in header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_header = data\n",
    "for i, h in enumerate(header + ['tag']):\n",
    "    data_with_header = data_with_header.withColumnRenamed('_c{}'.format(i), h)\n",
    "data_with_header.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DataFrames have schema - information about columns in dataframe. You can view it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All of the columns have string type - thats because we read them from csv and didn't use inferSchema flag. Lets cast continuous columns ourselves.\n",
    "\n",
    "To do this, we use spark column functions. \n",
    "Column represents a column in a Dataset that holds a Catalyst Expression that produces a value per row.\n",
    "You can construct Column insatance from it's name using pyspark.sql.functions.col and then call different functions on it, including cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cast_if_continuous(col_name, t):\n",
    "    if t == u'continuous':\n",
    "        return sf.col(col_name).cast('float').alias(col_name)\n",
    "    else:\n",
    "        return sf.col(col_name)\n",
    "\n",
    "data_with_types = data_with_header.select([cast_if_continuous(h, t) for h, t in zip(header, types)] + ['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have apropriate types in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_types.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You also can do different transformations on columns. For example let's calculate mean error rate for each column.\n",
    "\n",
    "There are several ways to introduce new column into our dataframe.\n",
    "One of them is to use .withColumn, which accepts column expression and column name.\n",
    "\n",
    "Another is to use .select with different column expressions as arguments.\n",
    "Expressions also could be strings or constants, which internally transforms to columns using sf.col or sf.lit (literal value).\n",
    "To provide a name for new column, you can call .alias on column.\n",
    "\n",
    "You can use '\\*' wildcard to select all columns in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_er_df = data_with_types.select('tag', sf.col('protocol_type'), \n",
    "                         ((sf.col('dst_host_serror_rate') + \n",
    "                           sf.col('dst_host_srv_serror_rate') +\n",
    "                           sf.col('dst_host_rerror_rate') + \n",
    "                           sf.col('dst_host_srv_rerror_rate') / 4).alias('mean_err_rate')))\n",
    "mean_er_df.orderBy('mean_err_rate', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's a lot easier to do aggregations on data using DataFrame API, because sf module also provides so called aggregate functions, which can be used with .groupby.\n",
    "\n",
    "Let's calculate the same statistic as in RDD API\n",
    "\n",
    "First, group data by two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_df = data_with_types.groupBy('tag', 'protocol_type')\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, aggregate it with corresponding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pt_df = grouped_df.agg(sf.count('protocol_type').alias('count'))\n",
    "pt_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to values we got in previous seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocols_by_type = {'back._tcp': 2203,  'buffer_overflow._tcp': 30,  'ftp_write._tcp': 8,  'guess_passwd._tcp': 53,  'imap._tcp': 12,  'ipsweep._icmp': 1153,  'ipsweep._tcp': 94,  'land._tcp': 21,  'loadmodule._tcp': 9,  'multihop._tcp': 7,  'neptune._tcp': 107201,  'nmap._icmp': 103,  'nmap._tcp': 103,  'nmap._udp': 25,  'normal._icmp': 1288,  'normal._tcp': 76813,  'normal._udp': 19177,  'perl._tcp': 3,  'phf._tcp': 4,  'pod._icmp': 264,  'portsweep._icmp': 1,  'portsweep._tcp': 1039,  'rootkit._tcp': 7,  'rootkit._udp': 3,  'satan._icmp': 3,  'satan._tcp': 1416,  'satan._udp': 170,  'smurf._icmp': 280790,  'spy._tcp': 2,  'teardrop._udp': 979,  'warezclient._tcp': 1020,  'warezmaster._tcp': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "protocols_by_type3 = {'{}_{}'.format(r['tag'], r['protocol_type']):r['count'] for r in pt_df.collect()}\n",
    "assert protocols_by_type3 == protocols_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As an exercise, calculate mean size (scr_bytes column) of payload for each tag. List of aggregate functions can be found [here](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Task 1\n",
    "mean_src_bytes_by_tag_df = ...\n",
    "mean_src_bytes_by_tag = ...\n",
    "assert mean_src_bytes_by_tag['teardrop.'] == 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "It's also possible to run SQL queries against dataframes. You must first create temproary view, that is virtual table to run your query against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_with_types.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use SparkSession to query your temproary view. You get back another DataFrame as a result, so you can run more computatations using either sql or regular API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_df = ss.sql('select * from data')\n",
    "sql_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the same stuff for the fourth time, now using the power of SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protocols_by_type_sql = ss.sql('''select tag, protocol_type, count(protocol_type) as count \n",
    "                                  from data group by tag, protocol_type''')\n",
    "protocols_by_type_sql.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even compare execution plans for both dataframes and confirm that they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protocols_by_type_sql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "With RDD you can simply pass your function to .map method, but with DataFrame you need to provide type information for spark to be able to use your function in experssions. Luckily, it is very easy in general\n",
    "\n",
    "You just need to wrap you function with functions.udf (stands for user-defined function) and spark will do the rest.\n",
    "\n",
    "For example, let's say you want to mirror protocol name for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_string(value):\n",
    "    return value[::-1]\n",
    "\n",
    "reverse_string_udf = sf.udf(reverse_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use your udf in expressions, applying it to columns and get another column in return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reversed_protocol_column = reverse_string_udf(sf.col('protocol_type'))\n",
    "reversed_protocol_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_df.withColumn('protocol_type', reversed_protocol_column).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDFs can accept multiple columns as arguments. Now, write a udf to extract key from row to complete our verification for sql dataframe\n",
    "\n",
    "(Note how you can use functions.udf as a decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sf.udf\n",
    "def key(...):\n",
    "    # task 2\n",
    "    ...\n",
    "\n",
    "    \n",
    "protocols_by_key = protocols_by_type_sql.withColumn('key', key(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protocols_by_type4 = {r['key']: r['count'] for r in protocols_by_key.collect()}\n",
    "assert protocols_by_type4 == protocols_by_type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
