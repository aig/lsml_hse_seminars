{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setting up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2.1\"\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.10.4-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkConf = pyspark.SparkConf() \\\n",
    "    .set(\"spark.executor.memory\", \"2560m\")\\\n",
    "    .set(\"spark.driver.memory\", \"2560m\")\\\n",
    "    .set(\"spark.yarn.executor.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.yarn.driver.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.python.worker.memory\", \"1536m\")\\\n",
    "    .set(\"spark.executor.instances\", 11)\\\n",
    "    .set(\"spark.default.parallelism\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(\n",
    "    master='yarn-client',\n",
    "    appName='seminar4',\n",
    "    conf=sparkConf\n",
    ")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "port = sc.uiWebUrl.split(':')[-1]\n",
    "print 'http://cluster1:{}'.format(port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! wget \"http://files.grouplens.org/datasets/movielens/ml-latest.zip\" -O \"/data/movielens.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! unzip /data/movielens.zip -d /data/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir sem4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "for csv in os.listdir('/data/movielens/ml-latest/'):\n",
    "    subprocess.check_output('hdfs dfs -put /data/movielens/ml-latest/{} sem4/'.format(csv), shell=True)\n",
    "    print csv, 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More FUN with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Assume we need to build a recommender system for films. First, we will need to generate some features for each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's read user ratings. By using some reader parameters, we can infer column names and types from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read(name):\n",
    "    return ss.read.csv('sem4/{}'.format(name), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratings = read('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We already know how to calculate mean movie rating, let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "movie_mean_rating = ratings.groupby('movieId').agg(sf.mean('rating'))\n",
    "movie_mean_rating.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's read movie descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data = read('movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have only 3 columns, one with id and two other are not very useful for any model in their current form. Let's build our feature space.\n",
    "\n",
    "First, we need to add rating data to our dataframe. We can acheive that with join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data = movie_data.join(movie_mean_rating, on='movieId', how='inner')\n",
    "movie_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Join have two parameters, besides the second dataframe. 'on' is a column name, instance of Column or a list of any of the above. 'how' is the type of join, more on that can be found [here](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Execution plan and persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, as you remember, Spark uses lazy evaluation, which means there were no real computations (except for .show) yet. But, you can view future physical plan of evaluation. Sometimes it's useful for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Okay, we've got some heavy computations here. Remember, each time you call an action on dataframe, Spark executes the plan from the very beginning. Sometimes it can reuse already computed data from cache, but it's preferable to explicitly state that you want your dataframe persisted. This will guarantee that the first time you call some action on any descendant of this dataframe, Spark will save intermediate results and use them, when you call action next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data = movie_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    ".cache() is shortcut for .persist() with Memory storage level. More on that topic [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#which-storage-level-to-choose)\n",
    "\n",
    "Another approach is to use .checkpoint(). This will force Spark to write full intermediate results to disk (you need to setup checkpoint dir first). It will actually change physical plan of that dataframe to 'read from disk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('chkp')\n",
    "movie_data.checkpoint().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generating simple features\n",
    "\n",
    "Ok, now lets turn those two string columns into something useful. As we can see, title column also contains release year. \n",
    "\n",
    "If we were using RDD API, we would use .map to do it. In DataFrame API, it can be done via User Defined Functions.\n",
    "\n",
    "If you call pyspark.sql.functions.udf on your regular function, it will turn it into UDF, and then you can use it in your expressions. Now it will accept columns as arguments and return other column.\n",
    "\n",
    "Let's extract title without year from title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def title(t):\n",
    "    return '('.join(t.split('(')[:-1])\n",
    "\n",
    "title_udf = sf.udf(title)\n",
    "movie_data = movie_data.withColumn('short_title', title_udf('title'))\n",
    "movie_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Also, udf can be used as a decorator. Now, write an udf that will extract year information. Remember, not every title has year at the end. You can return -1 in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@sf.udf\n",
    "def get_year(title):\n",
    "    pass\n",
    "\n",
    "movie_data = movie_data.withColumn('year', get_year('title').cast('integer'))\n",
    "movie_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's do something with that genres column. Let's create a boolean column for each genre, which is true if movie belongs to that genre. First, we need to create a list of all genres.\n",
    "\n",
    "As you can see, if film has multiple genres, they are divided by '|'. So, we can use sf.split to get an array of genres.\n",
    "\n",
    "By the way, columns can have complex types, like array, map, vector etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_data.select(sf.split('genres', '\\|')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After that we need 'explode' those arrays (which is equivalent to .flatMap in RDD API) and collect unique genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "col = sf.explode(sf.split('genres', '\\|'))\n",
    "genres = movie_data.select(col.alias('genre'))\n",
    "genres = list(genres.drop_duplicates().toPandas()['genre'])\n",
    "genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's create a column for each genre using .like (regular expression) with genre name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for g in genres:\n",
    "    movie_data = movie_data.withColumn(g, sf.col('genres').like(g))\n",
    "movie_data = movie_data.drop('genres')\n",
    "movie_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Window aggregations\n",
    "\n",
    "Sometimes you need to calculate some aggregations, which are dependent on values in your data. For example, for each row difference between some column value and maximum of that column. Normally, first you need to compute that maximum and then use it in expressions, but with window functions that could be avoided. \n",
    "\n",
    "At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the Frame. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try and calculate for each movie the number of movies of the same genre, that were released in previous two years. This feature can show how popular the genre was at the time of film release.\n",
    "Assume same genre means 'same set of genres', so we will create a boolean array from each genre column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_genres_col = sf.array(map(sf.col, genres)).alias('gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we'll define WindowSpecification. It'll tell Spark how to build Frame for each movie. We need that frame to include all films, which has the same value in 'gs' column and year between (current_film_year - 2) and current_film_year.\n",
    "\n",
    "Then, we select needed columns and call sf.count on movieId, adding .over(w) to use our window specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy('gs')\\\n",
    "          .orderBy('year')\\\n",
    "          .rangeBetween(-2, 0)\n",
    "movie_data.select('movieId', 'title', 'year', all_genres_col)\\\n",
    "    .withColumn('release_before_count', sf.count('movieId').over(w) - 1)\\\n",
    "    .filter(sf.col('year') != -1).orderBy('year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As an exercise, for each film calculate difference between it's rating and maximum rating of films with same set of genres and that were released between 5 years before and 5 years after film release year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ws = ...\n",
    "rd_col = ...\n",
    "movie_data.select('movieId', 'title', 'year', 'avg(rating)', all_genres_col)\\\n",
    "    .withColumn('rating_diff', rd_col)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Broadcasts and accumulators\n",
    "\n",
    "Here is a little example on how to use broadcasts and accumulators.\n",
    "\n",
    "Broadcast are useful when you have some heavy stateless object, that you need in your compulatons, or you don't want to recreate something for each partition (like compiled regexp). You just need call sc.broadcast on your object, and then call .value inside worker code to access it.\n",
    "\n",
    "Accumulators are useful when you need to get some statistics, while you compute something else. Remember, accumulators will not be filled until you call some action on your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import re\n",
    "devil_re = sc.broadcast(re.compile('devil', re.IGNORECASE))\n",
    "\n",
    "acc = sc.accumulator(0)\n",
    "\n",
    "@sf.udf \n",
    "def short_title_with_acc_bc(x):\n",
    "    if devil_re.value.search(x):\n",
    "        acc.add(1)\n",
    "    return title(x)\n",
    "\n",
    "\n",
    "md = movie_data.withColumn('title2', short_title_with_acc_bc('title'))\n",
    "print acc.value\n",
    "md.collect()\n",
    "print acc.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
