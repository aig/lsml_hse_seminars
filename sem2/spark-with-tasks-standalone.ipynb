{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets\n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Formally, an RDD is a read-only, partitioned collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "There are two ways to create RDDs − parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
    "\n",
    "Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up SparkContext\n",
    "SparkContext (aka Spark context) is the heart of a Spark application.\n",
    "\n",
    "You could also assume that a SparkContext instance is a Spark application.\n",
    "\n",
    "Spark context sets up internal services and establishes a connection to a Spark execution environment.\n",
    "\n",
    "Once a SparkContext is created you can use it to create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext is stopped).\n",
    "\n",
    "A Spark context is essentially a client of Spark’s execution environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os, sys\n",
    "#os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2.1\"\n",
    "#sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "#sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.10.4-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkConf = pyspark.SparkConf() \\\n",
    "#    .set(\"spark.executor.memory\", \"2560m\")\\\n",
    "#    .set(\"spark.driver.memory\", \"2560m\")\\\n",
    "#    .set(\"spark.yarn.executor.memoryOverhead\", 3584)\\\n",
    "#    .set(\"spark.yarn.driver.memoryOverhead\", 3584)\\\n",
    "#    .set(\"spark.python.worker.memory\", \"1536m\")\\\n",
    "#    .set(\"spark.executor.instances\", 11)\\\n",
    "#    .set(\"spark.default.parallelism\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other configuration properties can be found [here](https://spark.apache.org/docs/latest/configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = pyspark.SparkContext(\n",
    "    #master='yarn-client',\n",
    "    #appName='seminar3-rdd',\n",
    "    #conf=sparkConf\n",
    "#)\n",
    "sc = pyspark.SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#port = sc.uiWebUrl.split(':')[-1]\n",
    "#print 'http://cluster1:{}'.format(port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data Files\n",
    "\n",
    "In this notebook, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDD Cup 1999 competition dataset is described in detail \n",
    "[here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\" -O \"./kddcup.data_10_percent.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! hdfs dfs -put /data/kddcup.data_10_percent.gz ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RDD from a File\n",
    "The most common way of creating an RDD is to load it from a file. Notice that Spark's textFile can handle compressed files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = 'kddcup.data_10_percent.gz'\n",
    "raw_data = sc.textFile('file://' + os.path.abspath(os.curdir) + '/' + data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data file loaded into the raw_data RDD.\n",
    "\n",
    "Without getting into Spark transformations and actions, the most basic thing we can do to check that we got our RDD contents right is to count() the number of lines loaded from the file into the RDD and check a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating an RDD is to parallelize an already existing list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list = sc.parallelize([x + 5 for x in range(100)])\n",
    "print(rdd_list.count())\n",
    "rdd_list.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Basic Operations\n",
    "This section will introduce three basic but essential Spark operations. Two of them are the transformations map and filter. The other is the action collect. At the same time we will introduce the concept of persistence in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The filter Transformation\n",
    "This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a functions is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return True.\n",
    "\n",
    "For example, imagine we want to count how many normal. interactions we have in our dataset. We can filter our raw_data RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "normal_raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map Transformation\n",
    "By using the map transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular.\n",
    "\n",
    "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "csv_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlatMap transformation\n",
    "By using flatMap you can map each row to multiple new rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([1, 2, 3])\n",
    "copies = texts.flatMap(lambda x: [x for _ in range(x)])\n",
    "copies.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using map to create PairRDD\n",
    "If you have a tuple of length 2 as your RDD data type, you can use \\*ByKey operations on your RDD, with first value of tuple being the key and second being the value. Let's create such RDD.\n",
    "\n",
    "Of course we can use predefined functions with map and not just lambda. Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. normal) and the value is the whole list of elements that represents the row in the CSV formatted file. We could proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change key with standard map function. Let's say we want to aggregate data by tag and protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_key(x):\n",
    "    tag = x[0]\n",
    "    proto = x[1][1]\n",
    "    return '{}_{}'.format(tag, proto), 1\n",
    "\n",
    "type_protocol = key_csv_data.map(protocol_key)\n",
    "protocols_by_type = dict(type_protocol.reduceByKey(lambda x, y: x + y).collect())\n",
    "protocols_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antother way to acheive this is to use groupBy functions. In this case we get iterable with values corresponding to each key as second tuple value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by = key_csv_data.groupByKey()\n",
    "grouped_by.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can map values to desired statistic. Write a function that will get us same results as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_counter(values):\n",
    "    # Task 1\n",
    "    pass\n",
    "\n",
    "assert protocol_counter([(0, 'udp'), (0, 'udp'), (0, 'tcp')])['udp'] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_by_type2 = dict(grouped_by.mapValues(protocol_counter).collect())\n",
    "assert protocols_by_type2['normal.']['udp'] == protocols_by_type['normal._udp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Word Count in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['Apache Spark has as its architectural foundation the resilient distributed dataset RDD a read only multiset of data items distributed over a cluster of machines that is maintained in a fault tolerant way 2 The Dataframe API was released as an abstraction on top of the RDD followed by the Dataset API In Spark 1 x the RDD was the primary application programming interface API but as of Spark 2 x use of the Dataset API is encouraged 3 even though the RDD API is not deprecated 4 5 The RDD technology still underlies the Dataset API 6 7 ',\n",
    " 'Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm which forces a particular linear dataflow structure on distributed programs MapReduce programs read input data from disk map a function across the data reduce the results of the map and store reduction results on disk Spark s RDDs function as a working set for distributed programs that offers a deliberately restricted form of distributed shared memory 8 ',\n",
    " 'Spark facilitates the implementation of both iterative algorithms which visit their data set multiple times in a loop and interactive exploratory data analysis i e the repeated database style querying of data The latency of such applications may be reduced by several orders of magnitude compared to Apache Hadoop MapReduce implementation 2 9 Among the class of iterative algorithms are the training algorithms for machine learning systems which formed the initial impetus for developing Apache Spark 10 ',\n",
    " 'Apache Spark requires a cluster manager and a distributed storage system For cluster management Spark supports standalone native Spark cluster Hadoop YARN or Apache Mesos 11 For distributed storage Spark can interface with a wide variety including Alluxio Hadoop Distributed File System HDFS 12 MapR File System MapR FS 13 Cassandra 14 OpenStack Swift Amazon S3 Kudu or a custom solution can be implemented Spark also supports a pseudo distributed local mode usually used only for development or testing purposes where distributed storage is not required and the local file system can be used instead in such a scenario Spark is run on a single machine with one executor per CPU core ',\n",
    " 'Spark Core',\n",
    " 'Spark Core is the foundation of the overall project It provides distributed task dispatching scheduling and basic I O functionalities exposed through an application programming interface for Java Python Scala and R centered on the RDD abstraction the Java API is available for other JVM languages but is also usable for some other non JVM languages such as Julia 15 that can connect to the JVM This interface mirrors a functional higher order model of programming a driver program invokes parallel operations such as map filter or reduce on an RDD by passing a function to Spark which then schedules the function s execution in parallel on the cluster 2 These operations and additional ones such as joins take RDDs as input and produce new RDDs RDDs are immutable and their operations are lazy fault tolerance is achieved by keeping track of the lineage of each RDD the sequence of operations that produced it so that it can be reconstructed in the case of data loss RDDs can contain any type of Python Java or Scala objects ',\n",
    " 'Besides the RDD oriented functional style of programming Spark provides two restricted forms of shared variables broadcast variables reference read only data that needs to be available on all nodes while accumulators can be used to program reductions in an imperative style 2 ',\n",
    " 'A typical example of RDD centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones Each map flatMap a variant of map and reduceByKey takes an anonymous function that performs a simple operation on a single data item or a pair of items and applies its argument to transform an RDD into a new RDD '\n",
    "]\n",
    "\n",
    "text_rdd = sc.parallelize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dict(word_count.collect())['spark'] == 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
